import re
from onnx import ModelProto
import streamlit as st
import sys
import time
from datetime import datetime
from langchain_classic.retrievers.document_compressors.cross_encoder_rerank import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_community.chat_models import ChatOllama
from streamlit import session_state as ss
from streamlit.runtime.scriptrunner import get_script_run_ctx
import os
import ui_utils
import chat_manager

current_dir = os.path.dirname(os.path.abspath(__file__))
# å°†å…¶åŠ å…¥åˆ°ç³»ç»Ÿè·¯å¾„ä¸­
sys.path.append(current_dir)
import rag
import utils

# --- App Configuration ---
st.set_page_config(page_title="ChatXFEL Beta 1.0", page_icon='./draw/logo.png', layout='wide')

# --- Header ---
st.header('ChatXFEL: Q & A System for XFEL')

# --- CSS Styling (ä¿æŒ new_chatxfel_app çš„æ ·å¼ä¼˜åŒ–) ---
st.markdown(
    """
    <style>
    /* è°ƒæ•´ä¾§è¾¹æ å®½åº¦ */
    [data-testid="stSidebar"] {
        min-width: 320px !important;
        width: 320px !important;
    }
    /* ä¾§è¾¹æ æŒ‰é’®å¾®è°ƒ */
    [data-testid="stSidebar"] button {
        padding-left: 0.5rem;
        padding-right: 0.5rem;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# --- Session State Initialization ---
if 'agree' not in ss:
    ss['agree'] = False
if 'rewrite_stage' not in ss:
    ss['rewrite_stage'] = False      # æ ‡è¯†å½“å‰æ˜¯å¦å¤„äºâ€œç­‰å¾…ç”¨æˆ·ç¡®è®¤Queryâ€çš„çŠ¶æ€
if 'temp_query' not in ss:
    ss['temp_query'] = ""            # å­˜å‚¨ä¸­é—´ç”Ÿæˆçš„é‡å†™ç»“æœ
if 'confirmed_query' not in ss:
    ss['confirmed_query'] = ""       # å­˜å‚¨ç”¨æˆ·æœ€ç»ˆç¡®è®¤çš„é‡å†™ç»“æœ

# --- Agreement Logic ---
def update_agree():
    ss['agree'] = True
    
if not ss['agree']:
    with st.empty():
        msg = '''This page is an intelligent system to answer the questions in the field of XFEL. If you click the **agree box** below, 
        :red[you IP and the time will be recorded]. If you don't agree with that, please close the page. 
        This note will appear again when you refresh the page.''' 
        st.markdown(msg)
    agree = st.checkbox('Agree', key='read', value=False, on_change=update_agree)
    while True:
        time.sleep(3600)

def reset_retriever_cache():
    try:
        get_retriever.clear()
        get_retriever_runtime.clear()
    except Exception as e:
        pass

def clear_chat_history():
    # æ¸…ç©ºå½“å‰å¯¹è¯åŠç›¸å…³çŠ¶æ€
    ss.messages = [{"role": "assistant", "content": "What do you want to know about XFEL?"}]
    ss.rewrite_stage = False
    ss.temp_query = ""
    ss.confirmed_query = ""

# --- Dialogs (ä¿ç•™ new_chatxfel_app çš„å¼¹çª—é€»è¾‘) ---
@st.dialog("âš ï¸ Confirm Deletion")
def open_delete_dialog(chat):
    st.write(f"Are you sure you want to permanently delete **{chat['title']}**?")
    st.warning("This action cannot be undone.")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        if st.button("Cancel", use_container_width=True):
            st.rerun() 
            
    with col2:
        if st.button("Delete", type="primary", use_container_width=True):
            chat_id_to_delete = chat['id']
            current_id = st.session_state.current_chat_id
            
            # 1. ç‰©ç†ç§»é™¤
            st.session_state.chat_history = [c for c in st.session_state.chat_history if c['id'] != chat_id_to_delete]
            
            # 2. åˆ¤æ–­é€»è¾‘
            if chat_id_to_delete == current_id:
                if not st.session_state.chat_history:
                    chat_manager.create_new_chat()
                else:
                    new_target_id = st.session_state.chat_history[0]['id']
                    chat_manager.switch_chat(new_target_id)
            
            # 3. åˆ·æ–°é¡µé¢
            st.rerun()

# --- Sidebar ---
with st.sidebar:
    st.title('ChatXFEL Beta 1.0')
    st.markdown('[ChatXFELç®€ä»‹ä¸æé—®æŠ€å·§](https://confluence.cts.shanghaitech.edu.cn/pages/viewpage.action?pageId=129762874)')
    st.markdown('**é‡è¦æç¤ºï¼šå¤§æ¨¡å‹çš„å›ç­”ä»…ä¾›å‚è€ƒï¼Œç‚¹å‡»SourcesæŸ¥çœ‹å‚è€ƒæ–‡çŒ®**')
    
    # --- Settings & Filters (ä¿ç•™ new_chatxfel_app çš„æŠ˜å è®¾è®¡) ---
    with st.expander("âš™ï¸ Settings & Filters", expanded=False):
        st.caption("Configure Model & Search")
        
        model_list = ['Qwen3-30B']
        col_list = ['xfel_bibs_collection', 'xfel_bibs_collection_with_abstract', 'xfel_imported_v1','fix_with_abstract_only']
        embedding_list = ['BGE-M3']

        selected_model = st.selectbox('LLM model', model_list, index=0, key='selected_model')
        n_recall = 6 if selected_model.startswith('Q') else 5

        selected_em = st.selectbox('Embedding model', embedding_list, key='selected_em')
        if selected_em == 'llama2-7b':
            col_list.append('llama2_7b')
        elif selected_em == 'llama3-8b':
            col_list.append('llama3_8b')
        selected_col = st.selectbox('Bibliography collection', col_list, key='select_col', on_change=reset_retriever_cache)
        col_name = selected_col
        
        if col_name == 'book':
            st.info('Collection: Theses from EuXFEL.')
        if col_name == 'chatxfel':
            st.info('Collection: 3000+ publications (slower).')
        if col_name == 'report':
            st.info('Collection: Unpublished references (CDR, TDR).')

        st.caption("Filters")
        filter_year = st.checkbox('Filter by year', key='filter_year', value=True)
        year_start = 1949
        year_end = datetime.now().year
        
        if filter_year:
            min_year = 1949
            max_year = datetime.now().year
            c_y1, c_y2 = st.columns([1,1])
            year_start = c_y1.selectbox('Start', list(range(min_year, max_year+1))[::-1], key='year_start', index=max_year-2000)
            year_end = c_y2.selectbox('End', list(range(year_start, max_year+1))[::-1], key='year_end')
            
        filter_keyword = st.checkbox('Filter by keywords', key='filter_keyword', value=False)
        keyword_expr = ""

        if filter_keyword:
            key_input = st.text_input('Keywords in title', key='key_title', placeholder='e.g. XFEL, laser')
            if key_input:
                keywords = [k.strip() for k in key_input.split(',') if k.strip()]
                if keywords:
                    sub_exprs = [f'title like "%{k}%"' for k in keywords]
                    keyword_expr = f"({' or '.join(sub_exprs)})"
        
        # Filters é€»è¾‘æ„å»º
        filters = {}
        expr_parts = []
        if filter_year:
            expr_parts.append(f'(year >= {year_start} and year <= {year_end})')
        if keyword_expr:
            expr_parts.append(keyword_expr)
        if expr_parts:
            filters['expr'] = " and ".join(expr_parts)

        enable_abstract_routing = st.checkbox('Abstract Routing', value=False, help="First search abstracts to find relevant papers.")
        n_batch, n_ctx, max_tokens = 512, 8192, 8192 
        return_source = True
        use_mongo = True
        enable_log = st.checkbox('Enable log', key='log', value=True)
        use_monog = False
        
        # Response Mode
        response_mode = st.select_slider(
            'Response Mode',
            options=['Strict (Rigorous)', 'Balanced', 'Creative (Flexible)'],
            value='Balanced',
            help="Strict: Only answers from papers. Creative: Uses AI knowledge."
        )

    # --- Chat Management ---
    chat_manager.init_session()
    
    if st.button('â• New Chat', use_container_width=True):
        chat_manager.create_new_chat()
        # Reset rewrite states on new chat
        ss.rewrite_stage = False
        ss.temp_query = ""
        ss.confirmed_query = ""
        st.rerun() 

    with st.expander("ğŸ•’ Chat History", expanded=True):
        if not st.session_state.chat_history:
            st.write("No history yet.")
        else:
            for i, chat in enumerate(st.session_state.chat_history):
                col_title, col_del = st.columns([0.8, 0.2])
                label = chat['title']
                if chat['id'] == st.session_state.current_chat_id:
                    label = f"ğŸŸ¢ {label}"
                
                with col_title:
                    if st.button(label, key=f"hist_{chat['id']}", use_container_width=True):
                        chat_manager.switch_chat(chat['id'])
                        ss.rewrite_stage = False # åˆ‡æ¢å¯¹è¯æ—¶é€€å‡ºé‡å†™çŠ¶æ€
                        st.rerun()
                
                with col_del:
                    if st.button("ğŸ—‘ï¸", key=f"del_btn_{chat['id']}"):
                        open_delete_dialog(chat)

    st.button('Clear Current Chat', on_click=clear_chat_history, use_container_width=True)
    st.divider() 

# --- Backend Resources (Cache) ---
@st.cache_resource
def get_embedding(embedding_model, n_ctx, n_gpu_layers=1):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting embedding...")
    if embedding_model == 'BGE-M3':
        embedding = rag.get_embedding_bge()
    return embedding
embedding = get_embedding(embedding_model=selected_em, n_ctx=n_ctx)

@st.cache_resource
def get_llm(model_name, num_predict, keep_alive, num_ctx=8192, temperature=0.0):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting LLM...")
    llm = rag.get_llm_ollama(model_name=model_name, num_predict=num_predict, 
                             keep_alive=keep_alive, num_ctx=num_ctx, temperature=temperature, base_url='http://10.15.102.186:9000')
    return llm
llm = get_llm(model_name=selected_model, num_predict=2048, keep_alive=-1)

with open('naive.pt', 'r') as f:
    prompt_template = f.read()

@st.cache_data
def get_prompt_template(template):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting prompt...")
    prompt = rag.get_prompt(template)
    return prompt
prompt = get_prompt_template(template=prompt_template)

@st.cache_resource
def get_rerank_model(model_name='', top_n=n_recall):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting rerank model...")
    if model_name == '':
        model_name = 'BAAI/bge-reranker-v2-m3'
    rerank_model = HuggingFaceCrossEncoder(model_name=model_name)
    compressor = CrossEncoderReranker(model=rerank_model, top_n=top_n)
    return compressor

connection_args = utils.get_milvus_connection()
@st.cache_resource
def get_retriever(connection_args, col_name, _embedding):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting retriever...")
    if selected_em in ['llama2-7b', 'llama3-8b']:
        retriever = rag.get_retriever(connection_args=connection_args, col_name=col_name,
                                      embedding=_embedding, use_rerank=False, return_as_retreiever=False)
    else:
        retriever = rag.get_retriever(connection_args=connection_args, col_name=col_name,
                                      embedding=_embedding, vector_field='dense_vector',
                                      use_rerank=False, return_as_retreiever=False)
    return retriever

@st.cache_resource
def get_retriever_runtime(_retriever_obj, _compressor, filters=None):
    base_retriever = None
    if hasattr(_retriever_obj, "as_retriever"):
        search_kwargs = {'k': 10}
        if filters:
            search_kwargs = {**search_kwargs, **filters}
        base_retriever = _retriever_obj.as_retriever(search_kwargs=search_kwargs)
    else:
        if filters and "expr" in filters:
            _retriever_obj.current_filter = filters["expr"]
        else:
            _retriever_obj.current_filter = ""
        base_retriever = _retriever_obj

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=_compressor,
        base_retriever=base_retriever
    )
    return compression_retriever

@st.cache_resource
def get_hybrid_retriever_obj(connection_args, col_name):
    return rag.get_hybrid_retriever(connection_args, col_name, top_k=10)

@st.cache_resource
def get_routing_retriever_obj(connection_args, col_name):
    return rag.get_routing_retriever(connection_args, col_name, top_k=10)

if selected_em == 'BGE-M3': 
    if enable_abstract_routing:
        print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: Using Abstract Routing Retriever...")
        retriever_obj = get_routing_retriever_obj(connection_args, selected_col)
    else:
        retriever_obj = get_hybrid_retriever_obj(connection_args, selected_col)
else:
    retriever_obj = get_retriever(connection_args, selected_col, embedding)

compressor = get_rerank_model(top_n=n_recall)
retriever = get_retriever_runtime(retriever_obj, compressor, filters=filters)

initial_message = {"role": "assistant", "content": "What do you want to know about XFEL?"}

# --- Load History ---
if "messages" not in ss.keys():
    current_history = ss.get('chat_history', [])
    current_id = ss.get('current_chat_id', None)
    target_chat = next((c for c in current_history if c['id'] == current_id), None)
    if target_chat:
        ss['messages'] = target_chat['messages']
    else:
        chat_manager.create_new_chat(reset_ui=True)

# --- Feedback Function ---
def log_feedback(feedback:dict, use_mongo):
    if feedback.get('Feedback', '') == '':
        feedback['Feedback'] = ss['feedback']+1
    utils.log_rag(feedback, use_mongo=use_mongo)

# --- Message Rendering (ä½¿ç”¨ new_chatxfel_app çš„é«˜çº§æ ·å¼) ---
for message in ss.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        
        c = st.columns([1.2, 1.2, 7.6]) 
        
        if 'source' in message.keys():
            # 1. Source æŒ‰é’®
            with c[0].popover('Sources'):
                st.markdown(message['source'])
            
            # 2. Copy æŒ‰é’® (ä¿ç•™é«˜çº§å¤åˆ¶åŠŸèƒ½)
            with c[1].popover("Copy"):
                st.caption("**Markdown (Original)**")
                st.code(message['content'], language='markdown')
                st.caption("**Plain Text (Cleaned)**")
                raw_text = message['content']
                plain = re.sub(r'\$\$[\s\S]*?\$\$', '', raw_text)
                plain = re.sub(r'\$.*?\$', '', plain)
                plain = re.sub(r'\*\*|__|\*|_|`|^#+\s*', '', plain, flags=re.MULTILINE)
                plain = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', plain)
                st.code(plain.strip(), language=None)

            # 3. Feedback æŒ‰é’®
            if message == ss.messages[-1]:
                if 'feedback' in ss:
                    ss['feedback'] = None
                with c[2]:
                    feedback = st.feedback('stars', key='feedback', on_change=log_feedback, args=({'Feedback':''}, use_mongo,))

# --- Logging Utils ---
@st.cache_data
def log_ip_time(session_id):
    ip = session.request.remote_ip
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: {ip} connected or refreshed!", flush=True)

ctx = get_script_run_ctx()
client_ip = ''
if ctx:
    session = st.runtime.get_instance().get_client(ctx.session_id)
    client_ip = session.request.remote_ip
    log_ip_time(ctx.session_id)

# --- Input Handling & Rewrite Initiation ---
question_time = ''
if question:= st.chat_input():
    if enable_log:
        question_time = time.strftime('%Y-%m-%d %H:%M:%S')
    ss.messages.append({"role": "user", "content": question})
    
    # æ•´åˆé€»è¾‘ï¼šè§¦å‘ Interactive Rewrite (æ¥è‡ª chatxfel_app)
    with st.spinner("Optimizing your query for XFEL database..."):
        ss.temp_query = rag.rewrite_query(question, llm) 
        ss.rewrite_stage = True
        ss.confirmed_query = "" # é‡ç½®ç¡®è®¤çŠ¶æ€

    chat_manager.save_current_chat()
    with st.chat_message("user"):
        st.write(question)
    st.rerun()

# Feedback session init
if 'feedback_good' not in ss:
    ss['feedback_good'] = None
if 'feedback_bad' not in ss:
    ss['feedback_bad'] = None

# --- Interactive Rewrite Stage (æ¥è‡ª chatxfel_app) ---
if ss.rewrite_stage:
    with st.chat_message("assistant", avatar="ğŸ”"):
        st.info("I have rewritten your query to improve search results. You can refine it further:")
        
        # 1. å…è®¸æ‰‹åŠ¨ä¿®æ”¹ Query
        ss.temp_query = st.text_area(
            "Refined Search Query (Full View):", 
            value=ss.temp_query,
            height=120,
            help="You can manually edit this text to precisely match your needs."
        )
        
        # 2. æ¥æ”¶ç”¨æˆ·åé¦ˆè¿›è¡Œå†æ¬¡ AI ä¿®æ”¹
        user_feedback = st.text_input("Provide feedback to AI for better rewriting (optional):", 
                                      placeholder="e.g. 'Focus on the detector part', 'Expand abbreviations'")
        
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.button("âœ… Confirm & Search", type="primary"):
                ss.confirmed_query = ss.temp_query # ä¿å­˜ç”¨æˆ·ç¡®è®¤çš„ Query
                ss.rewrite_stage = False # ç»“æŸé‡å†™é˜¶æ®µ
                st.rerun()
        with col2:
            if st.button("ğŸ”„ Refine with AI"):
                if user_feedback:
                    with st.spinner("Refining..."):
                        ss.temp_query = rag.rewrite_query_with_feedback(
                            ss.messages[-1]["content"], ss.temp_query, user_feedback, llm
                        )
                    st.rerun()
                else:
                    st.warning("Please enter feedback first.")
    # é˜»å¡åç»­ä»£ç ï¼Œç›´åˆ°ç”¨æˆ·ç¡®è®¤
    st.stop()

# --- Response Generation Logic (æ•´åˆå) ---
if ss.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ''
        source = ''
        source_docs = []
        
        # æ¢å¤åŸå§‹é—®é¢˜æ–‡æœ¬
        original_question = ss.messages[-1]["content"]

        with st.status("Thinking...", expanded=(response_mode == 'Strict')) as status:
            
            # ã€å…³é”®æ•´åˆç‚¹ã€‘ï¼šä½¿ç”¨ Interactive Rewrite ç¡®è®¤çš„ç»“æœï¼Œæˆ–è€…è‡ªåŠ¨ç”Ÿæˆ
            if ss.confirmed_query:
                rewritten_question = ss.confirmed_query
                status.write("âœ… Using confirmed rewritten query.")
            else:
                rewritten_question = rag.rewrite_query(original_question, llm)
                
            p = ' Please answer the question as detailed as possible and make up you answer in markdown format.'
            final_question = f"{rewritten_question}{p}"
            
            # --- æ ¸å¿ƒæ£€ç´¢ä¸ç”Ÿæˆé€»è¾‘ ---
            if response_mode == 'Strict':
                max_retries = 2
                current_q = final_question
                for i in range(max_retries + 1):
                    status.write(f"ğŸ” Retrieval Attempt {i+1}...")
                    res_raw = retriever.invoke(current_q)
                    rel = rag.grade_relevance(original_question, res_raw, llm)
                    
                    if rel == 'yes':
                        source_docs = res_raw
                        status.write("âœ… Relevant evidence found.")
                        break
                    elif i < max_retries:
                        status.write("âš ï¸ Low relevance. Rewriting query...")
                        # ä¸¥æ ¼æ¨¡å¼ä¸‹è‡ªåŠ¨é‡è¯•
                        current_q = rag.rewrite_query(f"Focus on factual details of: {original_question}", llm) + p
                    else:
                        source_docs = res_raw 
                
                status.write("âœï¸ Generating response...")
                response_data = rag.retrieve_generate(final_question, llm, prompt, retriever, return_source=True)
                
                status.write("ğŸ›¡ï¸ Checking for hallucinations...")
                hal = rag.grade_hallucination(response_data['answer'], source_docs, llm)
                if hal == 'no':
                    full_response = "âš ï¸ [Self-Correction] Based on the references, I cannot fully confirm the previous thought. " + response_data['answer']
                else:
                    full_response = response_data['answer']
                source_docs = response_data['context']

            elif response_mode == 'Creative':
                response_data = rag.retrieve_generate(final_question, llm, prompt, retriever, return_source=True)
                util = rag.grade_utility(response_data['answer'], llm)
                if util == 'no':
                    status.write("ğŸ’¡ No info in papers. Switching to internal knowledge...")
                    fallback_prompt = f"The following question cannot be answered by specific XFEL papers. Please answer using your internal scientific knowledge: {original_question}"
                    fallback_res = llm.invoke(fallback_prompt)
                    full_response = "ğŸ’¡ **Note: Based on internal AI knowledge (not found in current papers):**\n\n" + fallback_res.content
                    source_docs = []
                else:
                    full_response = response_data['answer']
                    source_docs = response_data['context']
            
            else: # Balanced
                response_data = rag.retrieve_generate(final_question, llm, prompt, retriever, return_source=True)
                full_response = response_data['answer']
                source_docs = response_data['context']
            
            status.update(label="Response Generated!", state="complete", expanded=False)

        # 2. æµå¼è¾“å‡º
        ui_utils.stream_output(placeholder, full_response)

        # 3. Source å¤„ç† (ä¿ç•™ new_chatxfel_app çš„ç¾è§‚ç‰ˆ)
        if return_source and source_docs:
            for i, c in enumerate(source_docs):
                source += f'{c.page_content}'
                title = c.metadata.get('title', c.metadata.get('source', 'Unknown Title'))
                doi = c.metadata.get('doi', '')
                journal = c.metadata.get('journal', '')
                year = c.metadata.get('year', '')
                page = c.metadata.get('page', '')
                
                if doi == '':
                    source += f'\n\n**Ref. {i+1}**: {title}, {journal}, {year}, page {page}'
                else:
                    source += f'\n\n**Ref. {i+1}**: {title}, {journal}, {year}, [{doi}](http://dx.doi.org/{doi}), page {page}'
                
                if i != len(source_docs)-1:
                    source += '\n\n'
            
            cols = st.columns([8,3])
            with cols[0].popover('Source'):
                st.markdown(source)

    # 4. ä¿å­˜ä¸æ—¥å¿—
    if return_source:
        message = {"role": "assistant", "content": full_response, "source": source}
    else:
        message = {"role": "assistant", "content": full_response}
        
    if enable_log:
        logs = {'IP': client_ip, 'Time': question_time, 'Model': selected_model, 
                'Mode': response_mode, 'Question': original_question, 'Answer': full_response, 'Source': source}
        utils.log_rag(logs, use_mongo=use_mongo)
    
    ss.messages.append(message)
    # å®Œæˆä¸€æ¬¡å¯¹è¯åï¼Œæ¸…ç† rewrite çŠ¶æ€ä»¥é˜²ä¸‡ä¸€
    ss.confirmed_query = ""
    chat_manager.save_current_chat()
    st.rerun()