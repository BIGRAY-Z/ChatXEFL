# 会议总结：ReprBasis 机械可解释性研究框架

**项目名称：** reprbasis  
**日期：** 2025年12月22日  
**会议主题：** 基于统一特征空间的 LLM/LVLM 解释与编辑框架设计  
**参会人：**  张瑞铭 史英栋

---

## 1. Question

**背景与当前核心问题**: 尽管已有工作SAE来进行特征提取，但目前可解释性研究陷入了出现了逐层孤立的困境：研究者通常为每一层单独训练一个 SAE，这导致特征在层与层之间无法对齐。我们缺乏一个统一的坐标系来观察一个语义概念如何从底层的词汇碎片，演化为中层的逻辑，并最终影响顶层的输出，这会给跨层干预和跨模型知识迁移增加困难。

基于此，我们想进行包括以下子问题的探究：

* **同一模型的残差流在不同深度是否共享一个潜在的语义流形？** 如果我们在全量残差流上训练一个Global SAE，它是否work？如果work，是否说明我们就可以找到一个通用的基底来捕捉跨层的特征复用等现象？
* **模型的内部计算空间是否本质上受到最后一步 Unembedding的强约束？** 特征在 LLM 中是以方向的形式存储的，且残差流可以看成简单的线性运算，那么最后线性运算后投影其实可以拆解成每一层进行乘运算后直接投影到Unembedding上我们预期的方向上。基于此，我们可以预期：模型的整个残差流空间，在几何结构上都是由Unembedding矩阵的方向定义的，也就是说其实模型在到Unembedding层之前就在用Unembedding的方式思考了.  
(blog:https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)
* **不同规模或不同架构的模型，在学习同一任务时，其内生特征空间是否可以通过线性变换实现对齐？** 如果上一个问题得到了证实，那么不同模型对于相同输入或同一概念，残差流的数值或许不同，但在各自unembedding坐标系下的相对位置应该是高度相似的，那么我们完全可以通过Procrustes analysis方法找到对齐方式。
    


---

## 2. Methodology


### I. 统一字典构建
传统方法通常为每一层训练独立的SAE，这导致特征空间碎片化。我们提出将全模型的残留流视为统一实体：包含Embedding层、各级Attention块、FFN块以及最后的 Unembedding层，并在跨层收集的激活值上训练一个Global SAE。

### II. 特征偏好得分
为了量化特定组件（如某个Head或FFN）对目标特征$f_{target}$的贡献，我们定义了特征偏好得分$P \in (0, 1)$：

#### A. 注意力机制交互
分析Query($f_q$)、Key($f_k$)与Value($f_v$)的协同作用（标准推理模型下：$f_q$=$f_k$=$f_v$）。对于单个Attention Head，其输出 $f_h$计算如下：
$$f_h = \text{softmax}\left(\frac{(W_Q f_q)(W_K f_k)^T}{\sqrt{d_k}}\right) (W_V f_v) W_O$$
该Head对目标特征的偏好得分为：
$$P(f_{target}, (f_k, f_v)) = \cos(f_{target}, f_h)$$

#### B. 前馈网络交互
对于FFN内部的激活值$f_a$：
$$f_a = W_2 \sigma(W_1 f_q)$$
其偏好得分为：
$$P(f_{target}, f_a) = \cos(f_{target}, f_a)$$

### III. 优化与梯度编辑
基于偏好得分，我们无需全局反向传播即可实现针对性的模型编辑（例如特征遗忘任务）：
* **局部损失函数**：定义为目标遗忘特征$f_{forget}$与当前组件特征的关联度：
    $$L = P(f_{forget}, f_q)$$
* **梯度计算与更新**：通过计算局部梯度$\frac{\partial L}{\partial q}$，实现参数高效的微调。

### IV. 特征提取与可解释性分析
利用优化后的字典与得分系统，提取出具有高可解释性的语义特征，并绘制特征在模型内部的交互路径图(用Circuit)。

## 3. Contributions

1.  **特征交互与提取方法论：** 开发一套能够追踪特征在不同模型块间转换与组合的分析流程。
2.  **参数高效更新机制：** 提出利用局部梯度更新权重的方法，避免全局损失函数和全量反向传播。旨在实现精确的知识更新，避免因全量优化带来的灾难性遗忘等问题。
同时本方法适用于在线学习场景，有高效地将少量新知识注入到大模型中的潜力。
3.  **整合资源：**
    * **字典训练数据集：** 针对SAE优化的数据集。
    * **评测基准：** 包括特征识别基准（关注可解释性与特征解耦）以及诸如IOI、Unlearning等任务（主要关注模型的逻辑偏好，而非传统knowledge editing所关注的事实性查询）。

---

## 4. Training Datasets

为在Instruct/Thinking模型上训练Global SAE，我们设计了多领域的混合数据集。  在训练数据采集阶段，我们弃用数据集提供的Ground Truth，让模型自主推理生成，从而获取模型在主动调用知识与逻辑时的内部激活状态：

| 领域 | 来源数据集 | 核心价值 |
| :--- | :--- | :--- |
| **数学推理** | `gsm8k`, `Calc-ape210k`, `NuminaMath-CoT` | 捕获多步推理与思维链（CoT）特征 |
| **多任务指令** | `BAAI/Infinity-Instruct` (660 tasks) | 提供多样化的指令遵循信号 |
| **代码生成** | `Magicoder-OSS-Instruct-75K` | 捕获逻辑结构与编程语言特征 |

---

## 5. Next Steps

1.  **数据集调研：** 分析题目的难度梯度及Chain-of-Thought的逻辑深度，调研 任务覆盖面，找出能代表通用语义理解等关注重点正确的基准任务
2.  **精读相关论文：** 跑通SAE原始代码以及仓库中基于qwen3-4b的全局SAE
